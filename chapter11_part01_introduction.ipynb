{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kriaz100/deep-learning-with-python-notebooks/blob/master/chapter11_part01_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJPaHXe_uq31"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc5cSEdXuq37"
      },
      "source": [
        "# Deep learning for text\n",
        "\n",
        "https://ufal.mff.cuni.cz/udpipe/2/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wKMPpb5uq38"
      },
      "source": [
        "## Natural-language processing: The bird's eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLvbLp9ruq38"
      },
      "source": [
        "## Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI_j6gsGuq39"
      },
      "source": [
        "### Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIioVcJouq39"
      },
      "source": [
        "### Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCC0ozh2uq3-"
      },
      "source": [
        "### Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcVcFDgmuq3-"
      },
      "source": [
        "### Using the TextVectorization layer\n",
        "- The Vectorizer class defined below has several methods: standardize, tokenize, make_vocabulary, encode, and decode. \n",
        "\n",
        "- These methods can be called on a dataset to perform standardization, tokenization, vocabulary indexing etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "NmYwyZDJuq3_"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quUQlprMuq4C",
        "outputId": "91111c53-ad42-4ce3-b6b4-7d27be3f3f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "# encoding a sentence\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMN2Gnnwuq4D",
        "outputId": "2806e5fb-8ab8-41fa-c7c8-1f7b844a0fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "# Decoding a sentence\n",
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practce, we use Keras <font color ='blue'>**`TextVectorization`** layer </font> instead of programming everything from scratch in python\n",
        "\n",
        "- The `TextVectorization` layer below is configured to return sequences of words encoded as integers indices (output_mode is set to 'int').\n",
        "\n",
        "- The layer will do the following:\n",
        "    - *standardization*: convert text to lowercase and remove punctuation\n",
        "    - *tokenization*: split on white space"
      ],
      "metadata": {
        "id": "LvNYgdbaV5nE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Dun6jrgKuq4E"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras `TextVectorization` layer is flexible enough to handle any  use case for standardization, or tokenization because it can incorporate **custom funtions for standardization and tokenization** (as shown below)."
      ],
      "metadata": {
        "id": "HtSkP-9xag6C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "mQUhUTUcuq4E"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string              \n",
        "import tensorflow as tf\n",
        "\n",
        "# define custom standardization function\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor) #convert string_tensors to lowercase\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\") #replace punctuation w empty string\n",
        "\n",
        "# define custom tokenization function (for splitting)\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n",
        "\n",
        "# incorporate custom standardization and tokenization fns into TextVectorization layer\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply `TextVectorization` layer\n",
        "\n",
        "- To index the vocabulary of a text corpus, call the `adapt()` method of the `TextVectorization` layer with a dataset object that yields strings.\n",
        "\n",
        "- The vocabulary for the layer must be either supplied on construction <font color='blue'>or learned via adapt()</font>. When this layer is adapted, it will analyze the dataset, determine the frequency of individual string values, and create a vocabulary from them."
      ],
      "metadata": {
        "id": "-fQ5B0fSbv6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "9AGT58bsuq4F"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b2kxPdOuq4F"
      },
      "source": [
        "**Displaying the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i9FsKtCuq4G",
        "outputId": "dc06cae9-e7c6-4282-d61f-1b2f834c7a52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the **vocabulary of a test sentence** using `TextVectorization` layer"
      ],
      "metadata": {
        "id": "_Da6Iy-CuOxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcPs_wsluq4G",
        "outputId": "b4f5d1c5-d53b-4206-ae13-8a7124dd014b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rup-4e0Suq4G",
        "outputId": "8a8f381a-e5ce-4bb8-f3bf-cd263c4df6e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT NOTE\n",
        "\n",
        "`TextVectorization` layer can't be executed on GPU (or TPU). This has implications for model training (see **SIDEBAR** in the book)."
      ],
      "metadata": {
        "id": "WhFc0byOzbNo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJMLu9Y9uq4H"
      },
      "source": [
        "## Two approaches for representing groups of words: <font color='blue'>Sets and sequences</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11JhRIfjuq4H"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vAYnZ_Xuq4H",
        "outputId": "49dc1dad-fa3c-4c59-8ab0-8350dbff4c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  14.1M      0  0:00:05  0:00:05 --:--:-- 16.1M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B6XzAIxWuq4I"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-yZddubuq4I",
        "outputId": "4546f785-75e5-4ad9-f27f-9fbe75c353ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing a validation set by setting apart 20% of the training text files in a new directory, **aclImdb/val**"
      ],
      "metadata": {
        "id": "yPsaIXePXO9Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UYb8m7g1uq4I"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "# moving 20% files from  training and validation directory\n",
        "# I added argument exist_ok=True to os.makedirs() to avoid error in case\n",
        "#  dir exists already \n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category, exist_ok=True)  #adding 'exist_ok=True'\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using text_dataset_from_director to create batched dataset of text files and their labels for a directory structure. This is similar to the utility, image_dataset_from_dircetory, which was used for image files in chapter 8."
      ],
      "metadata": {
        "id": "p5jE6j2zWBIp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN_TOxtPuq4J",
        "outputId": "b900ba19-3d47-4459-e596-fd18799a1fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32                     #choice of batch size\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxRn74zsuq4J"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**\n",
        "\n",
        "These datasets yield inputs that are <font color='blue'>TensorFlow tf.string tensors</font>, and targets that are <font color='blue'>int32 tensors</font> encoding the value \"0\" or \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E4WUHcpuq4J",
        "outputId": "14f36713-179f-45ec-ebcd-79e558f4d8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'This movie is very funny. Amitabh Bachan and Govinda are absolutely hilarious. Acting is good. Comedy is great. They are up to their usual thing. It would be good to see a sequel to this :)<br /><br />Watch it. Good time-pass movie', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syuaQE35uq4J"
      },
      "source": [
        "### Processing words as a set: The bag-of-words approach\n",
        "The simplest way to encode a piece of text for processing by a nmachine model is to discard order and treat it as a set (a \"bag\") of tokens. You could either look at individual owrds (unigrams), or try to recover some local order information by looking at groups of consecutive tokens (N-grams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JE1M2qsuq4K"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-N3BR2zuq4K"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**\n",
        "\n",
        "The datasets train_ds, val_ds, and test_da were created earlier with keras utility `text_dataset_from_directory`.\n",
        "\n",
        "We will use `lambda function` to multi-hot encode the text in these datasets. Let's we first go over a toy example of lamba function. \n",
        "\n",
        "**Toy example of lambda function: think λ(x)**\n",
        "1. Suppose, we want to define a function λ(x) which multiplies x by 2, ie. returns 2x. That is, **λ(x)= 2x**.\n",
        "      \n",
        "      This is how we will write it in python\n",
        "\n",
        "      `lambda x: 2x`\n",
        "\n",
        "2. We want to apply (map) the function λ(x) to all x such that x ϵ A =[1, 2, 3].  \n",
        "\n",
        "  For this purpose We will use `map()` function.\n",
        "\n",
        "          map(lambda x: 2x, A)\n",
        "\n",
        "3. Finally, we want to list all the results y ϵ Y where y = λ(x). We will use  `list()`. The full code is shown below:\n",
        "\n",
        "  ```\n",
        "    A =[1, 2, 3]\n",
        "    Y = list(map(lambda x: 2*x, A))\n",
        "    print(Y)\n",
        "  ```\n",
        "        Output:\n",
        "        [2, 4, 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the `TextVectorization` layer creates the multi-hot encoding with a vocabulary of 20,000 words.\n",
        "\n",
        "We will apply the `TextVectorization` layer as a `lambda function` on datasets `train_ds`, `val_ds`, and `test_ds`. This will multi-hot encode the text in all these data sets."
      ],
      "metadata": {
        "id": "Abb3AB6H-K2N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mEsRLq08uq4K"
      },
      "outputs": [],
      "source": [
        "# defining Keras TextVectorization layer, \n",
        "#   specify max_tokens in vocabulary\n",
        "#   multi-hot encoding. \n",
        "#   Default encoding is 1gram. If we want bi-grams then set ngrams=2\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "\n",
        "# Using lambda fn to create text only data sets from train_ds\n",
        "#    i.e. extracting only feature but no labels from examples.\n",
        "#    Use that data to index dataset vocabulary, via adapt() method.\n",
        "#    The vocabulary from text_only_train_ds  will be used in training \n",
        "#    bi-gram model in a latter section.\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "# Applying TextVectorization as lambda function\n",
        "# on features (x) only in each ds to encode 1-grams with max vocab=20,000 words\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJhrBtU9uq4K"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y8hWm8Quq4K",
        "outputId": "9b8a9029-dc27-4cfe-fca2-498cecafabd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ts9le6tuq4L"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UXtELHYGuq4L"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy6f2pd0uq4L"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4c0suv-uq4L",
        "outputId": "18d85303-db7f-477e-dfc7-69b701c94255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.3995 - accuracy: 0.8328 - val_loss: 0.2907 - val_accuracy: 0.8876\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2716 - accuracy: 0.9008 - val_loss: 0.2870 - val_accuracy: 0.8920\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2380 - accuracy: 0.9158 - val_loss: 0.3028 - val_accuracy: 0.8900\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2258 - accuracy: 0.9248 - val_loss: 0.3192 - val_accuracy: 0.8902\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2170 - accuracy: 0.9310 - val_loss: 0.3321 - val_accuracy: 0.8916\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2171 - accuracy: 0.9316 - val_loss: 0.3419 - val_accuracy: 0.8914\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2061 - accuracy: 0.9350 - val_loss: 0.3495 - val_accuracy: 0.8850\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2049 - accuracy: 0.9374 - val_loss: 0.3614 - val_accuracy: 0.8866\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2057 - accuracy: 0.9363 - val_loss: 0.3788 - val_accuracy: 0.8890\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2069 - accuracy: 0.9377 - val_loss: 0.3774 - val_accuracy: 0.8878\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2925 - accuracy: 0.8874\n",
            "Test acc: 0.887\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Got validation accuracy 0.883, and text accuracy 0.885"
      ],
      "metadata": {
        "id": "ekvjvRfwi0vt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPnPbBrPuq4M"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9t4R9S8uq4M"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**\n",
        "- Some atomic concepts cannot be represented by 1-grams. For example, \"United States\". The local order information is important in these cases, which may lead to choice of N-grams (usually bi-grams), instad of 1-gram.\n",
        "\n",
        "- To configure the `TextVectorization` layer to return **bigrams**, specify `ngrams=2` argument in `TextVectorization` layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "H9K-UOMuuq4M"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2, \n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TiIsTZMuq4M"
      },
      "source": [
        "**Training and testing the binary bigram model**\n",
        "\n",
        "- The textVectorization layer defined above creates bi-grams (ngrams=2). \n",
        "- The output mode of the layer  is tf-idf. When applied to a text dataset, it will create vocabulary as well as learn the tf-idf weights.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CQWovfXuq4M",
        "outputId": "9623e79b-1565-4e89-e2cd-17194acb7f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.3733 - accuracy: 0.8464 - val_loss: 0.2708 - val_accuracy: 0.8970\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2399 - accuracy: 0.9178 - val_loss: 0.2830 - val_accuracy: 0.8968\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1985 - accuracy: 0.9348 - val_loss: 0.2937 - val_accuracy: 0.9000\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1855 - accuracy: 0.9433 - val_loss: 0.3142 - val_accuracy: 0.8988\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1792 - accuracy: 0.9467 - val_loss: 0.3368 - val_accuracy: 0.8974\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1762 - accuracy: 0.9492 - val_loss: 0.3422 - val_accuracy: 0.8982\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1706 - accuracy: 0.9513 - val_loss: 0.3509 - val_accuracy: 0.8950\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1759 - accuracy: 0.9527 - val_loss: 0.3569 - val_accuracy: 0.8976\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1741 - accuracy: 0.9528 - val_loss: 0.3600 - val_accuracy: 0.8972\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1685 - accuracy: 0.9538 - val_loss: 0.3721 - val_accuracy: 0.8988\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.2731 - accuracy: 0.8972\n",
            "Test acc: 0.897\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigram model got validation accuracy 0.901 and test accuracy 0.895\n",
        "\n",
        "Recall that the 1-gram model had test accuracy 0.885. So bigrams did not offer a huge improvement in accuracy over 1-gram model."
      ],
      "metadata": {
        "id": "iMVETaL9jnls"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-emtpUHRuq4N"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding\n",
        "We can add a bit more information by including the frequency of the N-grams appearing in the document. \n",
        "\n",
        "This can be achieved by using `TextVectorization` layer and setting `output_mode=\"count\"`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-w651y1uq4N"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nrLvg5k7uq4N"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FspK72ucuq4N"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**\n",
        "\n",
        "The problem is that some words -- for eaxmple, 'a', 'the', 'is', 'and', 'are' -- may have high frequencies but they are pretty useless features for text classification. Therefore, we use normalization:\n",
        "\n",
        "- In particular, we use TD-IDF normalization, which stands for \"*term frequency, inverse document frequency*\"\n",
        "\n",
        "- Under TF-IDF each term is wighted by its frequency in the current document (\"term frequency\") and divided by how often the term comes up (in other documents) across the dataset.\n",
        "\n",
        "- It can be computed as follows:\n",
        "\n",
        "  ```\n",
        "      def tfidf(term, document, dataset):\n",
        "          term_freq = document.count(term)\n",
        "          doc_freq  = math.log(sum(doc.count(term) for doc in dataset) + 1)\n",
        "          retrun term_freq/doc_freq\n",
        "  ```\n",
        "- TF-IDF Normalization can be implemented with Keras `TextVectorization` layers by specifying `output_mode= \"tf_idf\"`\n",
        "- Any `adapt()` call to this particular `textVectorization` layer will not only create vocabulary but also learn `tf-idf weights`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See **SIDEBAR** listings 11.2 and 11.3\n",
        "\n",
        "<font color='red'>**This model must be run on cpu instead of GPU**.</font> \n",
        "\n",
        "\n",
        "\n",
        "So in the top menu bar of the cell, select `Runtime` >> `runtime type` >> set `hardware accelerator = NONE`.</font>\n",
        "\n",
        "<font color='red'>Before running the cell, the following pervious cells have to be run on cpu.\n",
        "- importing textVectorization layer from keras\n",
        "- importing and managing data sets\n",
        "- creating directories, sub-directories for train, val, test\n",
        "- get_model() utility\n",
        "\n",
        "<font color='red'>This means we have to re-run on cpu all the cells before the text cell: **Training and testing the binary unigram model**</font>\n"
      ],
      "metadata": {
        "id": "xoHLsexWRkpx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lyqKIIz1uq4N"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zNQNNuUuq4N"
      },
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EPB16fRuq4O",
        "outputId": "728ad22b-d818-4522-c704-5a4dd4aa2a04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 13s 19ms/step - loss: 0.4887 - accuracy: 0.7970 - val_loss: 0.3146 - val_accuracy: 0.8786\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.3160 - accuracy: 0.8733 - val_loss: 0.3232 - val_accuracy: 0.8818\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2745 - accuracy: 0.8891 - val_loss: 0.3362 - val_accuracy: 0.8822\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2667 - accuracy: 0.8898 - val_loss: 0.3399 - val_accuracy: 0.8826\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2490 - accuracy: 0.8971 - val_loss: 0.3469 - val_accuracy: 0.8754\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2342 - accuracy: 0.8979 - val_loss: 0.3591 - val_accuracy: 0.8762\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2321 - accuracy: 0.9018 - val_loss: 0.3789 - val_accuracy: 0.8774\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2214 - accuracy: 0.9052 - val_loss: 0.3909 - val_accuracy: 0.8718\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2230 - accuracy: 0.9054 - val_loss: 0.3914 - val_accuracy: 0.8718\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2167 - accuracy: 0.9067 - val_loss: 0.3919 - val_accuracy: 0.8552\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3016 - accuracy: 0.8873\n",
            "Test acc: 0.887\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives 0.887 accuracy on test dataset, which is less than the accuracy of binary bi-gram model."
      ],
      "metadata": {
        "id": "Jz9FHpmnRGZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='blue'>SIDEBAR</font>**\n",
        "\n",
        "- See SIDEBAR listings 11.2 and 11.3\n",
        "- If we implement the textVectorization as part of the tf.data pipeline, we have to use CPU.\n",
        "- But that creates another problem: when we move to production environment, we have to separately implement there the pipeline again. This can result in dicrepencies.\n",
        "- The alternative is to embedd the textVectorization inside the model. After all, it is a keras layer.\n"
      ],
      "metadata": {
        "id": "NdM04hyFVMG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also see **<font color='blue'>SIDEBAR</font>** on p 375 to see how to add textVectorization layer to the model."
      ],
      "metadata": {
        "id": "v-YkN1Dra3FS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MKjSCp4Muq4O"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FX--Axduq4O",
        "outputId": "d560bd77-1cbe-4a43-979f-26816953c95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93.54 percent positive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chapter11_part01_introduction.i",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}