{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kriaz100/deep-learning-with-python-notebooks/blob/master/chapter11_part01_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJPaHXe_uq31"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc5cSEdXuq37"
      },
      "source": [
        "# Deep learning for text\n",
        "\n",
        "https://ufal.mff.cuni.cz/udpipe/2/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wKMPpb5uq38"
      },
      "source": [
        "## Natural-language processing: The bird's eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLvbLp9ruq38"
      },
      "source": [
        "## Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI_j6gsGuq39"
      },
      "source": [
        "### Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIioVcJouq39"
      },
      "source": [
        "### Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCC0ozh2uq3-"
      },
      "source": [
        "### Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcVcFDgmuq3-"
      },
      "source": [
        "### Using the TextVectorization layer\n",
        "- The Vectorizer class defined below has several methods: standardize, tokenize, make_vocabulary, encode, and decode. \n",
        "\n",
        "- These methods can be called on a dataset to perform standardization, tokenization, vocabulary indexing etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NmYwyZDJuq3_"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quUQlprMuq4C",
        "outputId": "9debde58-afcf-4a04-baea-2ffd3309fc99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "# encoding a sentence\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMN2Gnnwuq4D",
        "outputId": "50cded06-8f8f-4ced-bbe3-bb3e0409f7f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "# Decoding a sentence\n",
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practce, we use Keras <font color ='blue'>**`TextVectorization`** layer </font> instead of programming everything from scratch in python\n",
        "\n",
        "- The `TextVectorization` layer below is configured to return sequences of words encoded as integers indices (output_mode is set to 'int').\n",
        "\n",
        "- The layer will do the following:\n",
        "    - *standardization*: convert text to lowercase and remove punctuation\n",
        "    - *tokenization*: split on white space"
      ],
      "metadata": {
        "id": "LvNYgdbaV5nE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dun6jrgKuq4E"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras `TextVectorization` layer is flexible enough to handle any  use case for standardization, or tokenization because it can incorporate **custom funtions for standardization and tokenization** (as shown below)."
      ],
      "metadata": {
        "id": "HtSkP-9xag6C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mQUhUTUcuq4E"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string              \n",
        "import tensorflow as tf\n",
        "\n",
        "# define custom standardization function\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor) #convert string_tensors to lowercase\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\") #replace punctuation w empty string\n",
        "\n",
        "# define custom tokenization function (for splitting)\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n",
        "\n",
        "# incorporate custom standardization and tokenization fns into TextVectorization layer\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply `TextVectorization` layer\n",
        "\n",
        "- To index the vocabulary of a text corpus, call the `adapt()` method of the `TextVectorization` layer with a dataset object that yields strings.\n",
        "\n",
        "- The vocabulary for the layer must be either supplied on construction <font color='blue'>or learned via adapt()</font>. When this layer is adapted, it will analyze the dataset, determine the frequency of individual string values, and create a vocabulary from them."
      ],
      "metadata": {
        "id": "-fQ5B0fSbv6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9AGT58bsuq4F"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b2kxPdOuq4F"
      },
      "source": [
        "**Displaying the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i9FsKtCuq4G",
        "outputId": "6d1dc119-0300-436f-80f7-8b501404196a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the **vocabulary of a test sentence** using `TextVectorization` layer"
      ],
      "metadata": {
        "id": "_Da6Iy-CuOxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcPs_wsluq4G",
        "outputId": "e6314f5c-040b-4b05-afef-7417c9e10220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rup-4e0Suq4G",
        "outputId": "7999c977-d1ac-4cf8-a8b3-58ac68f2b8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT NOTE\n",
        "\n",
        "`TextVectorization` layer can't be executed on GPU (or TPU). This has implications for model training (see **SIDEBAR** in the book)."
      ],
      "metadata": {
        "id": "WhFc0byOzbNo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJMLu9Y9uq4H"
      },
      "source": [
        "## Two approaches for representing groups of words: <font color='blue'>Sets and sequences</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11JhRIfjuq4H"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vAYnZ_Xuq4H",
        "outputId": "2ea54240-bc10-465a-99d2-6786454bbfc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  9694k      0  0:00:08  0:00:08 --:--:-- 18.0M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B6XzAIxWuq4I"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-yZddubuq4I",
        "outputId": "6b8d1c7c-b895-4631-c280-c3403f688374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing a validation set by setting apart 20% of the training text files in a new directory, **aclImdb/val**"
      ],
      "metadata": {
        "id": "yPsaIXePXO9Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UYb8m7g1uq4I"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "# moving 20% files from  training and validation directory\n",
        "# I added argument exist_ok=True to os.makedirs() to avoid error in case\n",
        "#  dir exists already \n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category, exist_ok=True)  #adding 'exist_ok=True'\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using text_dataset_from_director to create batched dataset of text files and their labels for a directory structure. This is similar to the utility, image_dataset_from_dircetory, which was used for image files in chapter 8."
      ],
      "metadata": {
        "id": "p5jE6j2zWBIp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN_TOxtPuq4J",
        "outputId": "bf28015c-7d6d-4705-f13f-f622e82ff51d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32                     #choice of batch size\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxRn74zsuq4J"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**\n",
        "\n",
        "These datasets yield inputs that are <font color='blue'>TensorFlow tf.string tensors</font>, and targets that are <font color='blue'>int32 tensors</font> encoding the value \"0\" or \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E4WUHcpuq4J",
        "outputId": "d8c7b774-a78c-483d-d4ef-5554266df988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"This Filmfour funded Sci-Fi movie is most definitely a must see. While it takes huge influence from The Manchurian Candidate and offers nothing new or original plot wise; it's handled with the utmost skill that it comes off as being fresh and inventive, despite it being basically a re-run of an earlier film. It's good to know that films like this are still being made (even if they aren't getting wide releases), and Cypher is refreshing for that reason. The plot twists and turns, which gives it an element of paranoia and also serves in keeping the audience on the edge of their seat while trying to figure out the meaning of Cypher's mystery. The plot follows Morgan Sullivan; a bored suburban man that decides to take a job with Digicorp that involves him listening to speeches from several rival companies and recording them for reasons, to him, unknown. However, his job is interrupted when he meets a mysterious young lady known as Rita...<br /><br />This film features a number of stark white backgrounds that give it a very surreal edge and blend well with it's apocalyptic imaging of the future. This gives the film a very odd look that sets it apart from the majority of other films of the same type, with it's only real close affiliate being Kubrick's A Clockwork Orange. The plot is also very efficient and ditches character development in favour of the more stylish - and more thrilling - plot developing. You never quite know where you are with the plot, which serves in making it all the more intriguing. The acting is largely good with a largely unknown cast backing up the team of stars; Jeremy Northam and Lucy Lui. Northam very much looks the part of the quiet and disheartened man at the centre of the tale, and does well with his role. Lucy Lui is an actress that has a resume that doesn't quite fit her talent, but she has a look about her that just fits this movie.<br /><br />Cypher is far from perfect as some of the sequences are illogical and at times it can be inconsistent; but on the whole, if you want an inventive recent Sci-Fi film; Cypher is the way to go.\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syuaQE35uq4J"
      },
      "source": [
        "### Processing words as a set: The bag-of-words approach\n",
        "The simplest way to encode a piece of text for processing by a nmachine model is to discard order and treat it as a set (a \"bag\") of tokens. You could either look at individual owrds (unigrams), or try to recover some local order information by looking at groups of consecutive tokens (N-grams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JE1M2qsuq4K"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-N3BR2zuq4K"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**\n",
        "\n",
        "The datasets train_ds, val_ds, and test_da were created earlier with keras utility `text_dataset_from_directory`.\n",
        "\n",
        "We will use `lambda function` to multi-hot encode the text in these datasets. Let's we first go over a toy example of lamba function. \n",
        "\n",
        "**Toy example of lambda function: think λ(x)**\n",
        "1. Suppose, we want to define a function λ(x) which multiplies x by 2, ie. returns 2x. That is, **λ(x)= 2x**.\n",
        "      \n",
        "      This is how we will write it in python\n",
        "\n",
        "      `lambda x: 2x`\n",
        "\n",
        "2. We want to apply (map) the function λ(x) to all x such that x ϵ A =[1, 2, 3].  \n",
        "\n",
        "  For this purpose We will use `map()` function.\n",
        "\n",
        "          map(lambda x: 2x, A)\n",
        "\n",
        "3. Finally, we want to list all the results y ϵ Y where y = λ(x). We will use  `list()`. The full code is shown below:\n",
        "\n",
        "  ```\n",
        "    A =[1, 2, 3]\n",
        "    Y = list(map(lambda x: 2*x, A))\n",
        "    print(Y)\n",
        "  ```\n",
        "        Output:\n",
        "        [2, 4, 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the `TextVectorization` layer creates the multi-hot encoding with a vocabulary of 20,000 words.\n",
        "\n",
        "We will apply the `TextVectorization` layer as a `lambda function` on datasets `train_ds`, `val_ds`, and `test_ds`. This will multi-hot encode the text in all these data sets."
      ],
      "metadata": {
        "id": "Abb3AB6H-K2N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mEsRLq08uq4K"
      },
      "outputs": [],
      "source": [
        "# defining Keras TextVectorization layer, \n",
        "#   specify max_tokens in vocabulary\n",
        "#   multi-hot encoding. \n",
        "#   Default encoding is 1gram. If we want bi-grams then set ngrams=2\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "\n",
        "# Using lambda fn to create text only data sets from train_ds\n",
        "#    i.e. extracting only feature but no labels from examples.\n",
        "#    use that data set to index dataset vocabulary, via adapt() method\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "# Applying TextVectorization as lambda function\n",
        "# on features (x) only in each ds to encode 1-grams with max vocab=20,000 words\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJhrBtU9uq4K"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y8hWm8Quq4K",
        "outputId": "450dd5ef-2646-4f0b-db3c-64234ff7a81a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ts9le6tuq4L"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UXtELHYGuq4L"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy6f2pd0uq4L"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4c0suv-uq4L",
        "outputId": "1a075259-2463-45bf-ac95-523f6ad20e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 13s 15ms/step - loss: 0.4004 - accuracy: 0.8331 - val_loss: 0.2951 - val_accuracy: 0.8842\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2740 - accuracy: 0.9005 - val_loss: 0.3007 - val_accuracy: 0.8856\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2457 - accuracy: 0.9143 - val_loss: 0.3187 - val_accuracy: 0.8856\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2361 - accuracy: 0.9213 - val_loss: 0.3449 - val_accuracy: 0.8822\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2284 - accuracy: 0.9272 - val_loss: 0.3456 - val_accuracy: 0.8924\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2146 - accuracy: 0.9312 - val_loss: 0.3651 - val_accuracy: 0.8822\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2100 - accuracy: 0.9321 - val_loss: 0.3762 - val_accuracy: 0.8806\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2111 - accuracy: 0.9326 - val_loss: 0.3837 - val_accuracy: 0.8798\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2119 - accuracy: 0.9362 - val_loss: 0.3938 - val_accuracy: 0.8846\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2088 - accuracy: 0.9372 - val_loss: 0.3945 - val_accuracy: 0.8838\n",
            "782/782 [==============================] - 11s 13ms/step - loss: 0.2931 - accuracy: 0.8846\n",
            "Test acc: 0.885\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Got validation accuracy 0.883, and text accuracy 0.885"
      ],
      "metadata": {
        "id": "ekvjvRfwi0vt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPnPbBrPuq4M"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9t4R9S8uq4M"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**\n",
        "- Some atomic concepts cannot be represented by 1-grams. For example, \"United States\". The local order information is important in these cases, which may lead to choice of N-grams (usually bi-grams), instad of 1-gram.\n",
        "\n",
        "- To configure the `TextVectorization` layer to return **bigrams**, specify `ngrams=2` argument in `TextVectorization` layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "H9K-UOMuuq4M"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2, \n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TiIsTZMuq4M"
      },
      "source": [
        "**Training and testing the binary bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CQWovfXuq4M",
        "outputId": "3756553f-f20c-48f5-f6e8-e61aabd7ca32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 11s 16ms/step - loss: 0.3811 - accuracy: 0.8414 - val_loss: 0.2813 - val_accuracy: 0.8868\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2402 - accuracy: 0.9146 - val_loss: 0.2888 - val_accuracy: 0.8990\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2020 - accuracy: 0.9309 - val_loss: 0.3093 - val_accuracy: 0.8996\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2004 - accuracy: 0.9378 - val_loss: 0.3197 - val_accuracy: 0.9014\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1791 - accuracy: 0.9469 - val_loss: 0.3415 - val_accuracy: 0.9006\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1725 - accuracy: 0.9499 - val_loss: 0.3506 - val_accuracy: 0.9002\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1747 - accuracy: 0.9515 - val_loss: 0.3656 - val_accuracy: 0.9004\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.1677 - accuracy: 0.9546 - val_loss: 0.3720 - val_accuracy: 0.9010\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.1673 - accuracy: 0.9556 - val_loss: 0.3683 - val_accuracy: 0.8966\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1627 - accuracy: 0.9567 - val_loss: 0.3805 - val_accuracy: 0.9018\n",
            "782/782 [==============================] - 11s 13ms/step - loss: 0.2740 - accuracy: 0.8952\n",
            "Test acc: 0.895\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigram model got validation accuracy 0.901 and test accuracy 0.895\n",
        "\n",
        "Recall that the 1-gram model had test accuracy 0.885. So bigrams did not offer a huge improvement in accuracy over 1-gram model."
      ],
      "metadata": {
        "id": "iMVETaL9jnls"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-emtpUHRuq4N"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding\n",
        "We can add a bit more information by including the frequency of the N-grams appearing in the document. \n",
        "\n",
        "This can be achieved by using `TextVectorization` layer and setting `output_mode=\"count\"`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-w651y1uq4N"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrLvg5k7uq4N"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FspK72ucuq4N"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**\n",
        "\n",
        "The problem is that some words ('a', 'the', 'is', 'and', 'are') may have high frequencies but they are pretty useless feature for classification.\n",
        "\n",
        "- So we use TD-IDF normalization, which stands for \"*term frequency, inverse document frequency*\"\n",
        "\n",
        "- Under TF-IDF the terms are wighted by its frequency in the current document (\"term frequency\") and divided by how often the term comes up (in other documents) across the dataset.\n",
        "\n",
        "- It can be computed as follows:\n",
        "\n",
        " ```\n",
        "def tfidf(term, document, dataset):\n",
        "    term_freq = document.count(term)\n",
        "    doc_freq  = math.log(sum(doc.count(term) for doc in dataset) + 1)\n",
        "    retrun term_freq/doc_freq\n",
        "```\n",
        "- TF-IDF Normalization can be implemented with Keras `TextVectorization` layers by specifying `output_mode= \"tf_idf\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyqKIIz1uq4N"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zNQNNuUuq4N"
      },
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EPB16fRuq4O"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKjSCp4Muq4O"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FX--Axduq4O"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chapter11_part01_introduction.i",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}