{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kriaz100/deep-learning-with-python-notebooks/blob/master/chapter11_part01_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJPaHXe_uq31"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc5cSEdXuq37"
      },
      "source": [
        "# Deep learning for text\n",
        "\n",
        "https://ufal.mff.cuni.cz/udpipe/2/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wKMPpb5uq38"
      },
      "source": [
        "## Natural-language processing: The bird's eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLvbLp9ruq38"
      },
      "source": [
        "## Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI_j6gsGuq39"
      },
      "source": [
        "### Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIioVcJouq39"
      },
      "source": [
        "### Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCC0ozh2uq3-"
      },
      "source": [
        "### Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcVcFDgmuq3-"
      },
      "source": [
        "### Using the TextVectorization layer\n",
        "- The Vectorizer class defined below has several methods: standardize, tokenize, make_vocabulary, encode, and decode. \n",
        "\n",
        "- These methods can be called on a dataset to perform standardization, tokenization, vocabulary indexing etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NmYwyZDJuq3_"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quUQlprMuq4C",
        "outputId": "7db478f2-a24e-4b6d-c24d-a11fbd048603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "# encoding a sentence\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMN2Gnnwuq4D",
        "outputId": "57f2d4d7-9f4b-45a9-b615-a1dd5930d00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "# Decoding a sentence\n",
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practce, we use Keras <font color ='blue'>**`TextVectorization`** layer </font> instead of programming everything from scratch in python\n",
        "\n",
        "- The `TextVectorization` layer below is configured to return sequences of words encoded as integers indices (output_mode is set to 'int').\n",
        "\n",
        "- The layer will do the following:\n",
        "    - *standardization*: convert text to lowercase and remove punctuation\n",
        "    - *tokenization*: split on white space"
      ],
      "metadata": {
        "id": "LvNYgdbaV5nE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dun6jrgKuq4E"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras `TextVectorization` layer is flexible enough to handle any  use case for standardization, or tokenization because it can incorporate **custom funtions for standardization and tokenization** (as shown below)."
      ],
      "metadata": {
        "id": "HtSkP-9xag6C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mQUhUTUcuq4E"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string              \n",
        "import tensorflow as tf\n",
        "\n",
        "# define custom standardization function\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor) #convert string_tensors to lowercase\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\") #replace punctuation w empty string\n",
        "\n",
        "# define custom tokenization function (for splitting)\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n",
        "\n",
        "# incorporate custom standardization and tokenization fns into TextVectorization layer\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply `TextVectorization` layer\n",
        "\n",
        "- To index the vocabulary of a text corpus, call the `adapt()` method of the `TextVectorization` layer with a dataset object that yields strings.\n",
        "\n",
        "- The vocabulary for the layer must be either supplied on construction <font color='blue'>or learned via adapt()</font>. When this layer is adapted, it will analyze the dataset, determine the frequency of individual string values, and create a vocabulary from them."
      ],
      "metadata": {
        "id": "-fQ5B0fSbv6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9AGT58bsuq4F"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b2kxPdOuq4F"
      },
      "source": [
        "**Displaying the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i9FsKtCuq4G",
        "outputId": "0a325021-cbf7-49b6-8b86-7bd7e2efcaf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the **vocabulary of a test sentence** using `TextVectorization` layer"
      ],
      "metadata": {
        "id": "_Da6Iy-CuOxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcPs_wsluq4G",
        "outputId": "a37ad945-656a-4ef2-cf17-33da6d6e8ed7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rup-4e0Suq4G",
        "outputId": "1f9849f8-d23f-4225-fd1e-079e1c85d3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT NOTE\n",
        "\n",
        "`TextVectorization` layer can't be executed on GPU (or TPU). This has implications for model training (see **SIDEBAR** in the book)."
      ],
      "metadata": {
        "id": "WhFc0byOzbNo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJMLu9Y9uq4H"
      },
      "source": [
        "## Two approaches for representing groups of words: <font color='blue'>Sets and sequences</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11JhRIfjuq4H"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vAYnZ_Xuq4H",
        "outputId": "d0d690b2-e61f-4d4b-d26b-aa0babbc2de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  11.7M      0  0:00:06  0:00:06 --:--:-- 18.0M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B6XzAIxWuq4I"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-yZddubuq4I",
        "outputId": "206401f5-bd5a-4a79-b8fb-9cfbd557061d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing a validation set by setting apart 20% of the training text files in a new directory, **aclImdb/val**"
      ],
      "metadata": {
        "id": "yPsaIXePXO9Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UYb8m7g1uq4I"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "# moving 20% files from  training and validation directory\n",
        "# I added argument exist_ok=True to os.makedirs() to avoid error in case\n",
        "#  dir exists already \n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category, exist_ok=True)  #adding 'exist_ok=True'\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using text_dataset_from_director to create batched dataset of text files and their labels for a directory structure. This is similar to the utility, image_dataset_from_dircetory, which was used for image files in chapter 8."
      ],
      "metadata": {
        "id": "p5jE6j2zWBIp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN_TOxtPuq4J",
        "outputId": "acb87565-a396-4764-a31c-566f7ac53255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32                     #choice of batch size\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxRn74zsuq4J"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**\n",
        "\n",
        "These datasets yield inputs that are <font color='blue'>TensorFlow tf.string tensors</font>, and targets that are <font color='blue'>int32 tensors</font> encoding the value \"0\" or \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E4WUHcpuq4J",
        "outputId": "50de955c-dcc1-4889-88da-0d248192ff15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"Deliverance is John Boorman's 1972 horror/thriller movie about a group of four Atlanta businessmen (Burt Reynolds, Jon Voight, Ned Beatty and Ronnie Cox), that undertake a canoe trip down the Cahulawassee River before the river is dammed. Along the way, a raft (no pun intended) of unpleasant things happen to the men. Despite the nasty happenings in this picture, Boorman captures the natural beauty of the river nicely. The location was really chosen well. Indeed, this would still be a very nice film to watch, had the canoe trip gone smoothly. The lush forests and gentle landscape only make the horror more horrible. Not only is the location scenic, but also beautifully shot thanks to cinematographer Vilmos Zsigmond. It has been said that Burt Reynolds' performance as the outdoorsman, Lewis, is the star acting role in this film. I reckon however, that Jon Voight steals the show with his role as the suburban family man, Ed, who is rapidly forced to change his demeanour in order to survive. In fact, the scene in which climbs the cliff was not a stuntman. It was Voight himself. To cut costs the filming wasn't insured and the actors did their own stunts. The soundtrack is particularly noteworthy. Eric Weissberg's and Steve Mendel's performance on guitar and banjo as part of the Duelling Banjos sequence remains one of the most awesome pieces of soundtrack in the history of cinema for the sheer intensity of its performance. At a couple of other points in the movie, we are treated to more, softer, banjo music which provides a very pleasant accompaniment to the trip down the river For all the good points of this film, I did find it a little lacking in purpose. It doesn't build suspense very well and it isn't really as gruesome as we have been led to believe. The plot itself is somewhat poor and it doesn't really go anywhere. Nevertheless, this movie has enough good points to get my recommendation. I did like it but for fans of gore, there isn't really much of it. None really, in fact. It isn't so much a horror film as an adventure film that turns a little bit sour. Think of it like Rambo: First Blood meets Three Men In A Boat. Look out for a very young Charley Boorman as Ed's son. I did like this movie, the soundtrack, cinematography and acting earns it a well deserved 7 out of 10.\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syuaQE35uq4J"
      },
      "source": [
        "### Processing words as a set: The bag-of-words approach\n",
        "The simplest way to encode a piece of text for processing by a nmachine model is to discard order and treat it as a set (a \"bag\") of tokens. You could either look at individual owrds (unigrams), or try to recover some local order information by looking at groups of consecutive tokens (N-grams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JE1M2qsuq4K"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-N3BR2zuq4K"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**\n",
        "\n",
        "The datasets train_ds, val_ds, and test_da were created earlier with keras utility `text_dataset_from_directory`.\n",
        "\n",
        "We will use `lambda function` to multi-hot encode the text in these datasets. Let's we first go over a toy example of lamba function. \n",
        "\n",
        "**Toy example of lambda function: think λ(x)**\n",
        "1. Suppose, we want to define a function λ(x) which multiplies x by 2, ie. returns 2x. That is, **λ(x)= 2x**.\n",
        "      \n",
        "      This is how we will write it in python\n",
        "\n",
        "      `lambda x: 2x`\n",
        "\n",
        "2. We want to apply (map) the function λ(x) to all x such that x ϵ A =[1, 2, 3].  \n",
        "\n",
        "  For this purpose We will use `map()` function.\n",
        "\n",
        "          map(lambda x: 2x, A)\n",
        "\n",
        "3. Finally, we want to list all the results y ϵ Y where y = λ(x). We will use  `list()`. The full code is shown below:\n",
        "\n",
        "  ```\n",
        "    A =[1, 2, 3]\n",
        "    Y = list(map(lambda x: 2*x, A))\n",
        "    print(Y)\n",
        "  ```\n",
        "        Output:\n",
        "        [2, 4, 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the `TextVectorization` layer creates the multi-hot encoding with a vocabulary of 20,000 words.\n",
        "\n",
        "We will apply the `TextVectorization` layer as a `lambda function` on datasets `train_ds`, `val_ds`, and `test_ds`. This will multi-hot encode the text in all these data sets."
      ],
      "metadata": {
        "id": "Abb3AB6H-K2N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mEsRLq08uq4K"
      },
      "outputs": [],
      "source": [
        "# defining Keras TextVectorization layer, providing maximum tokens in vocabulary\n",
        "# and multi-hot encoding\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "\n",
        "# Using lambda fn to create text only data sets from train_ds\n",
        "#    i.e. extracting only feature but no labels from examples.\n",
        "#    use that data set to index dataset vocabulary, via adapt() method\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "# Applying TextVectorization as lambda function\n",
        "# on features (x) only in each ds to encode 1-grams with max vocab=20,000 words\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJhrBtU9uq4K"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y8hWm8Quq4K",
        "outputId": "bc6f7226-b995-4132-e9d6-2dee9148e2c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ts9le6tuq4L"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXtELHYGuq4L"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy6f2pd0uq4L"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4c0suv-uq4L"
      },
      "outputs": [],
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPnPbBrPuq4M"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9t4R9S8uq4M"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9K-UOMuuq4M"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TiIsTZMuq4M"
      },
      "source": [
        "**Training and testing the binary bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CQWovfXuq4M"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-emtpUHRuq4N"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-w651y1uq4N"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrLvg5k7uq4N"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FspK72ucuq4N"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyqKIIz1uq4N"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zNQNNuUuq4N"
      },
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EPB16fRuq4O"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKjSCp4Muq4O"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FX--Axduq4O"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chapter11_part01_introduction.i",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}