{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kriaz100/deep-learning-with-python-notebooks/blob/master/chapter11_part02_sequence-models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbi5SlRM2FPd"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5SRqtVt2FPi"
      },
      "source": [
        "### Processing words as a sequence: The sequence model approach\n",
        "\n",
        "Instead of manually crafting order based features, we expose model to raw word sequences and let it figure out features on its own -- ie. leverage power of deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpXw9TkP2FPj"
      },
      "source": [
        "#### A first practical example\n",
        "Let's try the first sequence model in parcatice. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFwyDTQ82FPk"
      },
      "source": [
        "**Downloading the data**\n",
        "\n",
        "IMDB data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukRMCWl12FPk",
        "outputId": "52a83191-e483-496f-f03f-a103e188a5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  27.0M      0  0:00:02  0:00:02 --:--:-- 27.0M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrlWxqWw2FPm"
      },
      "source": [
        "**Preparing the data**\n",
        "\n",
        "Preparing the directory structure\n",
        "\n",
        "Moving datsets into relevant directories using Keras utility `text_dataset_from_directory`\n",
        "\n",
        "Dont forget to modify `os.mkdirs()` by adding `exist_ok=True`, in case the directory already exists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l86PYTNJ2FPm",
        "outputId": "b5560ab6-a440-4e6f-b300-3f0ef9a486a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category, exist_ok=True)  #adding 'exist_ok=True'\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-nNj7ay2FPn"
      },
      "source": [
        "**Preparing integer sequence datasets**\n",
        "\n",
        "First we prepare the datasets that return integer sequences.\n",
        "We use textVectorizationlayer with output mode=\"int\"\n",
        "\n",
        "\n",
        "We also limit vocabulary size to 600 (ignoring less frequently occuring words) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zr8FYNoq2FPo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM34sVcM2FPp"
      },
      "source": [
        "**A sequence model built on one-hot encoded vector sequence**\n",
        "\n",
        "- To convert integer sequences to vector sequence, we  one-hot-encoding `tf.one_hot()`\n",
        "\n",
        "- Next we add bidirectional LSTM on top of vectors `layers.Bidirectional(layers.LSTM(32))`\n",
        "- Dropout layer is also added `layers.Dropout()\n",
        "- Finally add a calssification layer `sigmoid`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L8so-j32FPq",
        "outputId": "19bd1dc4-d03b-4dfa-efd4-a7c961f1bc8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               5128448   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,128,513\n",
            "Trainable params: 5,128,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)         # one-hot-encoding\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)     # bi-directional LSTM\n",
        "x = layers.Dropout(0.5)(x)                              # dropout layer\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "140cRzl32FPr"
      },
      "source": [
        "**Training a first basic sequence model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk-nGTFx2FPr",
        "outputId": "954aa388-51db-4b4c-ffe5-3e09dbbf7b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 454s 712ms/step - loss: 0.5288 - accuracy: 0.7516 - val_loss: 0.4018 - val_accuracy: 0.8468\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 442s 707ms/step - loss: 0.3401 - accuracy: 0.8787 - val_loss: 0.3067 - val_accuracy: 0.8834\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 443s 708ms/step - loss: 0.2698 - accuracy: 0.9069 - val_loss: 0.2960 - val_accuracy: 0.8832\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 444s 711ms/step - loss: 0.2331 - accuracy: 0.9183 - val_loss: 0.2868 - val_accuracy: 0.8864\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 444s 710ms/step - loss: 0.2033 - accuracy: 0.9307 - val_loss: 0.3046 - val_accuracy: 0.8862\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 445s 712ms/step - loss: 0.1757 - accuracy: 0.9402 - val_loss: 0.3255 - val_accuracy: 0.8616\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 445s 712ms/step - loss: 0.1653 - accuracy: 0.9446 - val_loss: 0.3068 - val_accuracy: 0.8888\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 445s 712ms/step - loss: 0.1418 - accuracy: 0.9535 - val_loss: 0.3182 - val_accuracy: 0.8878\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 445s 711ms/step - loss: 0.1224 - accuracy: 0.9613 - val_loss: 0.3641 - val_accuracy: 0.8886\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 444s 711ms/step - loss: 0.1103 - accuracy: 0.9650 - val_loss: 0.3734 - val_accuracy: 0.8846\n",
            "718/782 [==========================>...] - ETA: 25s - loss: 0.3137 - accuracy: 0.8695"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using one-hot encoding to convert words to vectors wsa not a great idea. There is a better way, *word imbeddings*"
      ],
      "metadata": {
        "id": "pe4JUSCxTmI3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNLGdyFc2FPs"
      },
      "source": [
        "#### Understanding word embeddings\n",
        "- One-hot encoding is a way of feature engineering. One-hot vectors are orthogonal to each other. The different tokens are assumed to be independent of each other.  \n",
        "\n",
        "- But this is not true. Some words share information with each other. Words \"movie\" and \"film\" are interchangeable. They vectors cannot be orthogonal to each other. They should be the same vector or close enough (L2 distance or cosine distance between them should be less).\n",
        "\n",
        "- We expect geometric distance between any to word vectors to be related to semantic distance between them -- vector for related words should lie close to each other; and for words that mean different things, these vector shold be far away from each other.\n",
        "\n",
        "<font color='blue'>***Word embeddings*** are vector representations of words that achieve exactly this: they map human language into a structured geometric space</font>\n",
        "\n",
        "Word embedding high dimensional vectors that are dense (floating-point vectors), with lower dimensionality (256-dimensional, 512-dimensional, or 1024-dimensional). This contrasts with one-hot vectors that are sparse (mostly zeros) and high dimensions (e.g 20,000) are need to capture large (say 20,000 word) vocabulary. So word embeddings pack more information due to denseness.\n",
        "\n",
        "<font color='blue'>*The word embeddings are structural representations. Their structure is learned from data*</font>\n",
        "\n",
        "- Similar words get embedded in close locations. \n",
        "- specific directons in the embedding space are meaningful.\n",
        "\n",
        "**There are two ways to obtain word embeddings**\n",
        "\n",
        "  1.  Learn them from data, jointly with the main task such as document classification or sentiment prediction.\n",
        "\n",
        "  2.  Load them into your model word embeddings precomputed (learned) from a different machine learning task than the one you are trying to solve. These are called <font color='blue'>pre-trained word embeddings</font>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVGL7zw-2FPs"
      },
      "source": [
        "#### Learning word embeddings with the Embedding layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLxL55Nq2FPs"
      },
      "source": [
        "**Instantiating an `Embedding` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1PO_VYA2FPt"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgsqjALT2FPt"
      },
      "source": [
        "**Model that uses an `Embedding` layer trained from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wv-HG7Rd2FPt"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owg2k8Sw2FPu"
      },
      "source": [
        "#### Understanding padding and masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Wm9zej2FPu"
      },
      "source": [
        "**Using an `Embedding` layer with masking enabled**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OgTmT7S2FPv"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZwQYtNg2FPv"
      },
      "source": [
        "#### Using pretrained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-fUruGd2FPv"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcF4TMpX2FPw"
      },
      "source": [
        "**Parsing the GloVe word-embeddings file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDUxilrz2FPw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH7DqX8l2FPw"
      },
      "source": [
        "**Preparing the GloVe word-embeddings matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "votOemJq2FPw"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVU8prF12FPx"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN6cV1tr2FPx"
      },
      "source": [
        "**Model that uses a pretrained Embedding layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_k5tA1H2FPx"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chapter11_part02_sequence-models.i",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}